{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXSIGP3srsQYhYFGOFFP0t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9dOy82IyYlhQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "T-fWtt7fZBN0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Whisper model\n",
        "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O77Lqy4bd3Aj",
        "outputId": "683397c0-317a-4a0f-8ce6-9abc6d013fd1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WhisperForConditionalGeneration(\n",
              "  (model): WhisperModel(\n",
              "    (encoder): WhisperEncoder(\n",
              "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (embed_positions): Embedding(1500, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x WhisperEncoderLayer(\n",
              "          (self_attn): WhisperSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): WhisperDecoder(\n",
              "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
              "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x WhisperDecoderLayer(\n",
              "          (self_attn): WhisperSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): WhisperSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load audio file\n",
        "audio_input = \"Rev.mp3\"\n",
        "waveform, sample_rate = torchaudio.load(audio_input)\n",
        "\n",
        "print(sample_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GiOwRF2Zmdh",
        "outputId": "32510785-79df-48f7-fd71-daf460d2cf60"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \\Need to resample since whisper can mostly work with 16000\n",
        "if sample_rate != 16000:\n",
        "    transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "    waveform = transform(waveform)\n",
        "    sample_rate = 16000"
      ],
      "metadata": {
        "id": "4-i9Hv3ZfDh8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess audio\n",
        "inputs = processor(waveform.squeeze(0), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1W_c7N4fFQ4",
        "outputId": "eb37468f-f622-4b8a-a140-655ef4b7a6a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_features': tensor([[[-0.6748, -0.6748, -0.6748,  ..., -0.6748, -0.6748, -0.6748],\n",
            "         [-0.6748, -0.6748, -0.6748,  ..., -0.6748, -0.6748, -0.6748],\n",
            "         [-0.6748, -0.6748, -0.6748,  ..., -0.6748, -0.6748, -0.6748],\n",
            "         ...,\n",
            "         [-0.6748, -0.6748, -0.6748,  ..., -0.6748, -0.6748, -0.6748],\n",
            "         [-0.6748, -0.6748, -0.6748,  ..., -0.6748, -0.6748, -0.6748],\n",
            "         [-0.6748, -0.6748, -0.6748,  ..., -0.6748, -0.6748, -0.6748]]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform STT\n",
        "with torch.no_grad():\n",
        "  generated_ids = model.generate(**inputs, forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\"))\n",
        "\n",
        "print(generated_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozXsRLiQfJ3R",
        "outputId": "4498d160-bb89-4a02-c29b-f2b423056fa8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1911,    11,   393,   291,   976,   385,   257,  5353,   466,  1337,\n",
            "          1166,  7318,   293,   577, 25242,  2316,  1985,   293,   437,   307,\n",
            "           264,  4088,  9482,   926,   309,   293,  1338,    11,  1310,   577,\n",
            "          4825,    12,  8014,   338,  7318,  1985,    13]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode transcription\n",
        "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(\"Transcription:\", transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8oTmiWGfZWG",
        "outputId": "09d5ffdd-391f-44cc-8d38-a9a87c3bc02c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription:  Hey, can you give me a brief about generative AI and how diffusion model works and what is the transform architecture around it and yeah, maybe how multi-model AI works.\n"
          ]
        }
      ]
    }
  ]
}